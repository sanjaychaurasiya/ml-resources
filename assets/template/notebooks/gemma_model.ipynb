{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Started with Gemma Model"
      ],
      "metadata": {
        "id": "tAorUjDLYF98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing the Libraries"
      ],
      "metadata": {
        "id": "cFhjR2SVYOyV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFcMn6ZlYEls"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the Libraries"
      ],
      "metadata": {
        "id": "cAXXPIjGYVCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "3177JCdTYUN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "lE00HVgGZ-ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemma 7b Model"
      ],
      "metadata": {
        "id": "2RzMU05XonHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the Gemma 7b Parameter Model and its Tokenizer"
      ],
      "metadata": {
        "id": "KYlkz836Yf39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-7b\", device_map=\"auto\")"
      ],
      "metadata": {
        "id": "F3YjidfmYdem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making Prediction on the model"
      ],
      "metadata": {
        "id": "xjOuL9i9g9Th"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Write me a poem about Machine Learning.\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "BHJdkoH9YyP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "JPRf5TgndDeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Explain Large Language Model.\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "acoharzhdHTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "wEQEyrKfg6c3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs[0][0]"
      ],
      "metadata": {
        "id": "b4i6MNbWlhPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(outputs[0])"
      ],
      "metadata": {
        "id": "YW9y3vNLg73Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemma 2b parameter Model"
      ],
      "metadata": {
        "id": "RpgC2I--oiX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading Gemma 2b model and tokenizer"
      ],
      "metadata": {
        "id": "Lg8jKYlYoOmD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b-it\", device_map=\"auto\")"
      ],
      "metadata": {
        "id": "e2uUscPrlUOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making Prediction on Gemma 2b"
      ],
      "metadata": {
        "id": "SMCJo1OSocHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"Explain Large Language Model.\"\n",
        "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "9aHsy0kzoXNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model.generate(**input_ids)\n",
        "print(tokenizer.decode(outputs[0]))"
      ],
      "metadata": {
        "id": "xl18PDVbowz5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}