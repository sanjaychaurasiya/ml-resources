{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xu8pS3GOiJ"
      },
      "source": [
        "# Fine-tuning LLAMA2-7b using QLoRA method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIY_lkZQGcVe"
      },
      "source": [
        "### Installing the Libraries\n",
        "\n",
        "1. **`transformers:`**\n",
        "Transformers is a popular and powerful open-source library for natural language processing (NLP) tasks. It provides a wide range of pre-trained models, tokenizers, and tools for various NLP applications. Here's an overview of its features and uses:\n",
        "  * Pre-trained Models: Transformers offers a vast collection of pre-trained models, including BERT, GPT-2, and DistilBERT, which are trained on massive amounts of text data and can be fine-tuned for specific tasks.\n",
        "\n",
        "  * Model Architecture: It provides a variety of transformer architectures, such as encoder-decoder, decoder-only, and autoregressive, suitable for different NLP tasks like text classification, machine translation, and question answering.\n",
        "\n",
        "  * Tokenization: Transformers includes tokenization tools for processing and converting text into numerical representations that can be understood by transformer models.\n",
        "\n",
        "  * Fine-tuning: It provides easy-to-use tools for fine-tuning pre-trained models on specific datasets, allowing adaptation to new tasks and domains.\n",
        "\n",
        "  * Pipelines: Transformers offers pipelines for common NLP tasks, such as text classification, summarization, and question answering, simplifying the process of applying models to real-world applications.\n",
        "\n",
        "\n",
        "2. **`peft:`**\n",
        "PEFT (Parameter-Efficient Fine-tuning) is a library for fine-tuning large language models (LLMs) in a parameter-efficient manner. It provides a number of techniques for reducing the number of parameters in an LLM, including:\n",
        "  * Low-rank adaptation (LoRA): This technique replaces the weight matrices of an LLM with lower-rank matrices. This can significantly reduce the number of parameters without sacrificing accuracy.\n",
        "\n",
        "  * Prompt tuning: This technique fine-tunes an LLM by adding a small number of parameters to the model that are specifically designed for the task at hand.\n",
        "\n",
        "  * Weight quantization (QLoRA): This technique quantizes the weights of an LLM to a lower precision, such as 8-bit or 4-bit. This can further reduce the size of the model without sacrificing accuracy.\n",
        "\n",
        "\n",
        "3. **`accelerate:`** Accelerate is a library for distributed training of PyTorch models. It provides a number of features that can make training PyTorch models faster and easier, including:\n",
        "  * Distributed data parallel (DDP): This technique allows multiple GPUs to be used to train a PyTorch model.\n",
        "\n",
        "  * Gradient checkpointing: This technique saves intermediate activations of a PyTorch model, which can reduce the memory footprint of the model and make training faster.\n",
        "\n",
        "  * Mixed precision training: This technique uses a combination of single-precision and half-precision floating-point numbers to train a PyTorch model. This can make training faster without sacrificing accuracy.\n",
        "  \n",
        "  Accelerate can be used to train PyTorch models on a variety of hardware platforms, including CPUs, GPUs, and TPUs.\n",
        "\n",
        "\n",
        "4. **`bitsandbytes:`** Bitsandbytes is a library for quantizing PyTorch models. It provides a number of quantization algorithms that can be used to reduce the size of a PyTorch model without sacrificing accuracy.\n",
        "\n",
        "  Bitsandbytes can be used to quantize PyTorch models for a variety of tasks, including natural language processing, computer vision, and speech recognition. It can also be used to quantize PyTorch models to run on mobile devices or other resource-constrained environments.\n",
        "\n",
        "\n",
        "5. **`trl:`** TRL (Transformers Reinforcement Learning) is a library for training large language models (LLMs) using reinforcement learning (RL). It provides a number of features that can make training LLMs using RL easier, including:\n",
        "  * A variety of RL algorithms: TRL supports a variety of RL algorithms, including policy gradient, actor-critic, and proximal policy optimization.\n",
        "\n",
        "  * Integration with popular LLM frameworks: TRL can be used to train LLMs with a variety of popular LLM frameworks, including Hugging Face Transformers, OpenAI Gym, and DeepMind Lab.\n",
        "\n",
        "  * Easy-to-use API: TRL provides an easy-to-use API that makes it easy to train LLMs using RL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5BYRFYwGIpM",
        "outputId": "262e494f-1a5c-4a74-93fe-58fe04c0de32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m235.5/244.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.5/92.5 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m86.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c_AlryQGt1d"
      },
      "source": [
        "### Importing the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8k-t00ucGv5z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "# from datasets import load_dataset\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, HfArgumentParser, TrainingArguments, pipeline, logging\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqbZE94hQ6zj"
      },
      "source": [
        "### Setting the value of Hyperparamters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6IkowU1RET8"
      },
      "outputs": [],
      "source": [
        "# 1. The name of the model that you are going to tune\n",
        "model_name = \"NousResearch/llama-2-7b-chat-hf\"\n",
        "\n",
        "# 2. The name of the dataset to use\n",
        "# dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "\n",
        "# 3. Fine-tuned model name\n",
        "fine_tuned_model = \"llama-2-7b-mini-guanco\"\n",
        "\n",
        "# ####################### LoRA Parameters ########################\n",
        "\n",
        "# # 4. LoRa attention (rank) dimension\n",
        "# lora_r = 4\n",
        "\n",
        "# # 5. Alpha parameter for LoRA scaling\n",
        "# lora_alpha = 16\n",
        "\n",
        "# # 6. Dropout probability for LoRA layers\n",
        "# lora_dropout = 0.1\n",
        "\n",
        "# ####################### bitsandbytes Parameters ########################\n",
        "\n",
        "# 7. Activate 4bit precision base model loading\n",
        "use_4bit = True\n",
        "\n",
        "# # 8. Compute dtype for 4bit base models\n",
        "# bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "# # 9. Quantization type (fp4 or nf4)\n",
        "# bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "# # 10. Activate nested quantization for 4-bit base models (double quantization)\n",
        "# use_nested_quant = False\n",
        "\n",
        "# ####################### TrainingArguments Parameters ########################\n",
        "\n",
        "# 11. Output directory where the models predictions and model checkpoints will be stored\n",
        "output_dir = \"./results\"\n",
        "\n",
        "# # 12. Number of training epochs\n",
        "# num_epochs = 1\n",
        "\n",
        "# # 13. Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "# bf16 = False\n",
        "# fp16 = False\n",
        "\n",
        "# # 14. Batch size per GPU for training\n",
        "# per_device_train_batch_size = 1\n",
        "\n",
        "# # 15. Batch size per GPU for evalutation\n",
        "# per_device_evaluation_batch_size = 1\n",
        "\n",
        "# # 16. Number of updates steps to accumulate gradient for\n",
        "# gradient_accumulation_steps = 1\n",
        "\n",
        "# # 17. Enable gradient checkpointing\n",
        "# gradient_checkpointing = True\n",
        "\n",
        "# # 18. Max gradient normal (gradient cliping)\n",
        "# max_grad_norm = 0.3\n",
        "\n",
        "# # 19. Initial learning rate (AdamW optimizer)\n",
        "# learning_rate = 2e-4\n",
        "\n",
        "# # 20. Weight decay apply to all layers except bias/layer normalization weight\n",
        "# weight_decay = 0.001\n",
        "\n",
        "\n",
        "# # 23. Number of training steps (overrides train_epochs)\n",
        "# max_steps = -1\n",
        "\n",
        "# # 24. Ratio of steps for a linear warmup (from  0 to learning rate)\n",
        "# warmup_ratio = 0.03\n",
        "\n",
        "# # 25. Group sequences into batches with same length, saves memory and speeds up training considerably\n",
        "# group_by_length = True\n",
        "\n",
        "# # 26. Save checkpoing every X updates steps\n",
        "# save_steps = 25\n",
        "\n",
        "# # 27. Log every X updates steps\n",
        "# logging_steps = 25\n",
        "\n",
        "# ####################### SFT Parameters ########################\n",
        "\n",
        "# # 28. Maximum sequence length to use\n",
        "# max_sequence_length = None\n",
        "\n",
        "# # 29. Pack multiple short examples in the same input sequence to increase efficiency\n",
        "# packing = False\n",
        "\n",
        "# # 30. Load the entire model on GPU 0\n",
        "# device_map = {\"\": 0}# 21. Optimizer to use\n",
        "# optimizer = \"paged_adamw_32bit\"\n",
        "\n",
        "# # 22. Learning rate scheduler (constant a bit better than cosine)\n",
        "# lr_scheduler_type = \"constant\"\n",
        "\n",
        "# # 23. Number of training steps (overrides train_epochs)\n",
        "# max_steps = -1\n",
        "\n",
        "# # 24. Ratio of steps for a linear warmup (from  0 to learning rate)\n",
        "# warmup_ratio = 0.03\n",
        "\n",
        "# # 25. Group sequences into batches with same length, saves memory and speeds up training considerably\n",
        "# group_by_length = True\n",
        "\n",
        "# # 26. Save checkpoing every X updates steps\n",
        "# save_steps = 25\n",
        "\n",
        "# # 27. Log every X updates steps\n",
        "# logging_steps = 25\n",
        "\n",
        "# ####################### SFT Parameters ########################\n",
        "\n",
        "# # 28. Maximum sequence length to use\n",
        "# max_sequence_length = None\n",
        "\n",
        "# # 29. Pack multiple short examples in the same input sequence to increase efficiency\n",
        "# packing = False\n",
        "\n",
        "# # 30. Load the entire model on GPU 0\n",
        "# device_map = {\"\": 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaQCCeLYYqVr"
      },
      "source": [
        "### Downloading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "CT0jdWzQsl1-",
        "outputId": "6006ee63-9ae4-4fc8-9417-cc26aea26115"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6099c5d3-52b6-4875-9427-249ddcde3b9f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6099c5d3-52b6-4875-9427-249ddcde3b9f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "\n",
        "# Choose the kaggle.json file that you downloaded\n",
        "files.upload()\n",
        "\n",
        "# Make directory named kaggle and copy kaggle.json file there.\n",
        "!mkdir ~/.kaggle\n",
        "\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# Change the permissions of the file.\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "\n",
        "# !kaggle datasets list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLYuwbQnspD-",
        "outputId": "08686959-98e2-45db-af3e-1e7f148f94fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading hindi-english-parallel-corpus.zip to /content\n",
            " 85% 95.0M/112M [00:00<00:00, 193MB/s]\n",
            "100% 112M/112M [00:00<00:00, 197MB/s] \n"
          ]
        }
      ],
      "source": [
        "# Downloading the dataset\n",
        "!kaggle datasets download -d \"vaibhavkumar11/hindi-english-parallel-corpus\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCsqYNaPsquN",
        "outputId": "0338ba7f-a2d0-4734-c4bf-352d1c999f78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/hindi-english-parallel-corpus.zip\n",
            "  inflating: hindi_english_parallel.csv  \n"
          ]
        }
      ],
      "source": [
        "# Extracting the dataset\n",
        "!unzip \"/content/hindi-english-parallel-corpus.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1aUeqE3u0dt"
      },
      "source": [
        "### Loading the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "asCgENxFtqw2",
        "outputId": "0c30cc8f-6c17-4239-b550-1c07ed63102e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               hindi  \\\n",
              "0    अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें   \n",
              "1                    एक्सेर्साइसर पहुंचनीयता अन्वेषक   \n",
              "2              निचले पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
              "3               ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
              "4  उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...   \n",
              "\n",
              "                                          english  \n",
              "0  Give your application an accessibility workout  \n",
              "1               Accerciser Accessibility Explorer  \n",
              "2  The default plugin layout for the bottom panel  \n",
              "3     The default plugin layout for the top panel  \n",
              "4  A list of plugins that are disabled by default  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-26375dc7-a10b-4f1f-b46f-a512a8b9af62\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hindi</th>\n",
              "      <th>english</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें</td>\n",
              "      <td>Give your application an accessibility workout</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>एक्सेर्साइसर पहुंचनीयता अन्वेषक</td>\n",
              "      <td>Accerciser Accessibility Explorer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>निचले पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
              "      <td>The default plugin layout for the bottom panel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
              "      <td>The default plugin layout for the top panel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...</td>\n",
              "      <td>A list of plugins that are disabled by default</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26375dc7-a10b-4f1f-b46f-a512a8b9af62')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-26375dc7-a10b-4f1f-b46f-a512a8b9af62 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-26375dc7-a10b-4f1f-b46f-a512a8b9af62');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0c2fc955-743d-4dfe-bb10-996bdb619d0b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0c2fc955-743d-4dfe-bb10-996bdb619d0b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0c2fc955-743d-4dfe-bb10-996bdb619d0b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Setting the data directory\n",
        "data_dir = \"/content/hindi_english_parallel.csv\"\n",
        "\n",
        "# Loading the dataset\n",
        "dataset = pd.read_csv(data_dir)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "enGg7LrEm1pz",
        "outputId": "75b679fc-418a-40b2-bc4b-aa3f58ec0aaa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'एक्सेर्साइसर पहुंचनीयता अन्वेषक'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# type(dataset)\n",
        "dataset.iloc[1][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IJzmyFdjPV7",
        "outputId": "146bb7e3-13a6-4ced-ab0b-eaced3612de7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "hindi      6056\n",
              "english     725\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Checking for null values\n",
        "dataset.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StBjinW4kIid"
      },
      "outputs": [],
      "source": [
        "# Dropping the null values\n",
        "data = dataset.dropna(axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "avgmwQJlkNg4",
        "outputId": "de00d435-e013-4176-8a3c-88aa90ee48a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "hindi      0\n",
              "english    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "data.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lky-_C6Ik3rP"
      },
      "outputs": [],
      "source": [
        "# Selecting only the first 500 rows for fine tuning the model\n",
        "data = data[:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhxB37H_0HLt"
      },
      "source": [
        "### Preprocessing the Dataset\n",
        "\n",
        "\n",
        "In the case of Llama 2, the authors used the following Prompt template for the chat models:\n",
        "```\n",
        "<s>[INST] <<SYS>>\n",
        "System prompt\n",
        "<</SYS>>\n",
        "\n",
        "User prompt [/INST] Model answer </s>\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bd_oPEl2u_aC"
      },
      "outputs": [],
      "source": [
        "def transform(example):\n",
        "  text = f'<s>[INST] {example[\"hindi\"]} [/INST] {example[\"english\"]}</s>' #f\"<s>[INST] {example[\"hindi\"]} [/INST] {example[\"english\"]}</s>\"\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6qAzy6aYIpu"
      },
      "outputs": [],
      "source": [
        "# transformed_data = map(transform, data)\n",
        "# This will not work"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "transformed_data = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(data),\n",
        "})"
      ],
      "metadata": {
        "id": "ZBxHUetq8hTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bhERZ1De6ZO"
      },
      "source": [
        "### Loading bnb Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjcqkNstfSB_"
      },
      "outputs": [],
      "source": [
        "compute_dtype = getattr(torch, \"float16\")\n",
        "\n",
        "bnb_configuration = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                       bnb_4bit_quant_type=\"nf4\",\n",
        "                                       bnb_4bit_compute_dtype=\"float16\",\n",
        "                                       bnb_4bit_use_double_quant=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny3pPOqrg4Qr"
      },
      "source": [
        "### Checking GPU Compatibility with bfloat16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRgeSZpNg_9H"
      },
      "outputs": [],
      "source": [
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "  major, _ = torch.cuda.get_device_capability()\n",
        "  if major >= 8:\n",
        "    print(\"=\", * 80)\n",
        "    print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "    print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78X6YlNKh2oW"
      },
      "source": [
        "### Loading the LLAMA2-7b Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44e0ZHhBh6vW"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             quantization_config=bnb_configuration,\n",
        "                                             device_map={\"\": 0}) # Load the entire model on GPU 0\n",
        "\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1YhUppRif2l"
      },
      "source": [
        "### Loading the LLAMA Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpH6oYrhilSQ"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# Fix weird overflow issue with fp16 training\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51I8P-V-jDmw"
      },
      "source": [
        "### Loading the LoRA confgurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15aGebJcjHqs"
      },
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(lora_alpha=16,\n",
        "                         lora_dropout=0.1,\n",
        "                         r=64,\n",
        "                         bias=\"none\",\n",
        "                         task_type=\"CAUSAL_LM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_Iej5t3ji6I"
      },
      "source": [
        "### Setting Training Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CNiitHmYjn6w"
      },
      "outputs": [],
      "source": [
        "training_arguments = TrainingArguments(output_dir=output_dir,\n",
        "                                       num_train_epochs=5,\n",
        "                                       per_device_train_batch_size=4,\n",
        "                                       gradient_accumulation_steps=1,\n",
        "                                       optim=\"paged_adamw_32bit\",\n",
        "                                       save_steps=2,\n",
        "                                       logging_steps=2,\n",
        "                                       learning_rate=2e-4,\n",
        "                                       weight_decay=0.001,\n",
        "                                       fp16=False,\n",
        "                                       bf16=False,\n",
        "                                       max_grad_norm=0.3,\n",
        "                                       max_steps=-1,\n",
        "                                       warmup_ratio=0.03,\n",
        "                                      #  group_by_length=True,\n",
        "                                       lr_scheduler_type=\"constant\",\n",
        "                                      #  report_to=\"tensorboard\",\n",
        "                                       )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b-rSjiTlC5C"
      },
      "source": [
        "### Setting SFT (Supervised Fine Tuning) parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80Gm_P8UlLk0"
      },
      "outputs": [],
      "source": [
        "trainer = SFTTrainer(model=model,\n",
        "                     train_dataset=transformed_data[\"train\"],\n",
        "                     formatting_func=transform,\n",
        "                     peft_config=peft_config,\n",
        "                     dataset_text_field=\"text\",\n",
        "                    #  max_seq_length=None,\n",
        "                     tokenizer=tokenizer,\n",
        "                     args=training_arguments,\n",
        "                     packing=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nW1EWrJSlpta"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6KBqWfSlsaA"
      },
      "outputs": [],
      "source": [
        "# clear_cache()\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FG8TxqLnlwpX"
      },
      "source": [
        "### Save the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnaPBQODlyno"
      },
      "outputs": [],
      "source": [
        "trainer.model.save_pretrained(fine_tuned_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OslA9CQKABu0"
      },
      "source": [
        "### Making prediction before merging fine-tuned weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kd2AP_BAD54"
      },
      "outputs": [],
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"What is a large language model?\"\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKoUoqOBCGDx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5674450c-21d7-48dd-a3b2-8d18be0874ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>[INST] What is a large language model? [/INST]  A large language model is a type of artificial intelligence (AI) model that is trained on a large dataset of text to generate language outputs that are coherent and natural-sounding. everybody. These models are typically trained on vast amounts of text data, such as books, articles, and websites, and are designed to learn the patterns and structures of language.\n",
            "\n",
            "Large language models are often used for a variety of natural language processing (NLP) tasks, such as language translation, text summarization, and language generation. They are also used for more creative applications, such as writing poetry or stories, and even creating new languages.\n",
            "\n",
            "Some examples of large language models include:\n",
            "\n",
            "1. BERT (Bidirectional Encoder Representations from Transformers): A popular large language model developed by Google that is trained on a large dataset of text and can be fine-t\n"
          ]
        }
      ],
      "source": [
        "print(result[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"What is a large language model?\"\n",
        "pipe = pipeline(task=\"translation_en_to_hi\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "translation = pipe(f\"<s>[INST]Translate the sentence in Hindi:  {prompt} [/INST]\")\n"
      ],
      "metadata": {
        "id": "eS23o-lKQL4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(translation[0])"
      ],
      "metadata": {
        "id": "Fuqh7-sKQneD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eceb5f26-dbd5-4cd1-f321-6b1d4540ace9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'translation_text': '[INST]Translate the sentence in Hindi:  What is a large language model? [/INST]  In Hindi, the sentence \"What is a large language model?\" can be translated as:\\n everybody क्या एक बढ़ी भाषा मॉडल है?'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VHYwvBhAJiv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd4f652a-0947-4d9b-f0f7-20ae8db9776c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20933"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "# Empty VRAM\n",
        "del model\n",
        "del pipe\n",
        "del trainer\n",
        "import gc\n",
        "gc.collect()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reloading the model and merging the Weights"
      ],
      "metadata": {
        "id": "cBHliQ8h5lO9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpmoiDGAALev"
      },
      "outputs": [],
      "source": [
        "# Reload model in FP16 and merge it with LoRA weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    low_cpu_mem_usage=True,\n",
        "    return_dict=True,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map={\"\": 0}) # Load the entire model on GPU 0\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, fine_tuned_model)\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "# Reload tokenizer to save it\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After fine-tuning model's Results"
      ],
      "metadata": {
        "id": "QOsxx_qJ5y2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "# logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"What is a large language model?\"\n",
        "pipe = pipeline(task=\"translation_en_to_hi\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "translation = pipe(f\"<s>[INST]Translate the sentence in Hindi:  {prompt} [/INST]\")\n"
      ],
      "metadata": {
        "id": "GMNs9tUWTTXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(translation[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPr4gQK2pg2W",
        "outputId": "efa68dc6-7557-4cf6-cc94-0921942d8252"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'translation_text': '[INST]Translate the sentence in Hindi:  What is a large language model? [/INST]  In Hindi, the sentence \"What is a large language model?\" can be translated as:\\n\\nक्या है बढ़ा भाषा मौजूदा?'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "# logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"What is you name?\"\n",
        "pipe = pipeline(task=\"translation_en_to_hi\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "translation = pipe(f\"<s>[INST]Translate the sentence in Hindi:  {prompt} [/INST]\")\n",
        "print(translation[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VLmuLIUpm1l",
        "outputId": "ab698805-f03b-4e40-821a-bea3f7d77702"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'translation_text': '[INST]Translate the sentence in Hindi:  What is you name? [/INST]  In Hindi, the sentence \"What is your name?\" can be translated as:\\n\\nतुम्हारा नाम क्या है?'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ignore warnings\n",
        "# logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Run text generation pipeline with our next model\n",
        "prompt = \"Give your application an accessibility workout?\"\n",
        "pipe = pipeline(task=\"translation_en_to_hi\", model=model, tokenizer=tokenizer, max_length=200)\n",
        "translation = pipe(f\"<s>[INST]Translate the sentence in Hindi:  {prompt} [/INST]\")\n",
        "print(translation[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS9mLx0V0pc2",
        "outputId": "a9349eba-5d66-4e71-8d79-95c0bb2a85db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'translation_text': '[INST]Translate the sentence in Hindi:  Give your application an accessibility workout? [/INST]  In Hindi, the sentence \"Give your application an accessibility workout\" can be translated as:\\n\\nआपका ऐपलिकेशन अधिकारित करें कार्यक्रम [Aapka aplisikshan adhikariit karein karyakram]'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kLPq8qfC1X1k"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}